{
  "metadata": {
    "name": "sql_1v11",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Helpful Flink Resources\n\n* [Flink SQL API Examples. Common SQL query patterns. Windows, aggregation ...][1]\n* [AWS Online Tech Talks (YouTube). Streaming Analytics via. Kinesis Data Analytics Studio][2]\n* [Flink on Zeppelin 13 part YouTube Series][3]\n* [Flink on Zeppeling - Streaming Blog Post][4]\n* [Flink interpreter for Apache Zeppelin (Flink Documenation)][5]\n* [Flink SQL Documentation 1.11][6]\n\n[1]:https://awsfeed.com/whats-new/big-data/get-started-with-flink-sql-apis-in-amazon-kinesis-data-analytics-studio\n[2]:https://www.youtube.com/watch?v\u003dmRDst424mKY\n[3]:https://www.youtube.com/watch?v\u003dYxPo0Fosjjg\u0026list\u003dPL4oy12nnS7FFtg3KV1iS5vDb0pTz12VcX\n[4]:https://zjffdu.medium.com/flink-on-zeppelin-part-3-streaming-5fca1e16754\n[5]:http://zeppelin.apache.org/docs/0.9.0/interpreter/flink.html#streamexecutionenvironment-executionenvironment-streamtableenvir\n[6]:https://ci.apache.org/projects/flink/flink-docs-release-1.11/dev/table/sql/"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### [Flink\u0027s Zeppelin Interpreters][1] \n\n| Name | Class | Description\n| --- | ----------- | --------|\n| %flink | FlinkInterpreter | Creates ExecutionEnvironment/StreamExecutionEnvironment/BatchTableEnvironment/StreamTableEnvironment and provides a Scala environment\n| %flink.pyflink | PyFlinkInterpreter | Provides a python environment\n| %flink.ipyflink | IPyFlinkInterpreter | Provides an ipython environment\n| %flink.ssql | FlinkStreamSqlInterpreter | Provides a stream sql environment\n| %flink.bsql | FlinkBatchSqlInterpreter | Provides a batch sql environment\n\n[1]:http://zeppelin.apache.org/docs/0.9.0/interpreter/flink.html#overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Create Table in Glue Data Catalog\n\n* [Flink Supported Data Types][1]\n\n[1]:https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/types/"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\nDROP TABLE IF EXISTS yellow_cab;\n\nCREATE TABLE yellow_cab (\n   `VendorID` INT,\n   `tpep_pickup_datetime` TIMESTAMP,\n   `tpep_dropoff_datetime` TIMESTAMP,\n   `passenger_count` INT,\n   `trip_distance` FLOAT,\n   `RatecodeID` INT,\n   `store_and_fwd_flag` STRING,\n   `PULocationID` INT,\n   `DOLocationID` INT,\n   `payment_type` INT,\n   `fare_amount` FLOAT,\n   `extra` FLOAT,\n   `mta_tax` FLOAT,\n   `tip_amount` FLOAT,\n   `tolls_amount` FLOAT,\n   `improvement_surcharge` FLOAT,\n   `total_amount` FLOAT,\n   `congestion_surcharge` FLOAT,\n   WATERMARK FOR tpep_dropoff_datetime AS tpep_dropoff_datetime - INTERVAL \u00275\u0027 SECOND\n) \n WITH (\n   \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n   \u0027stream\u0027 \u003d \u0027yellow-cab-trip\u0027,\n   \u0027aws.region\u0027 \u003d \u0027us-east-1\u0027,\n   \u0027scan.stream.initpos\u0027 \u003d \u0027LATEST\u0027,\n   \u0027format\u0027 \u003d \u0027json\u0027\n);"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\nDESCRIBE yellow_cab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Select all Fields from the yellow-cab-trip stream"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT * FROM yellow_cab;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Filter"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT * FROM yellow_cab WHERE trip_distance \u003e 3.0 "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### [User Defined Function][1]\n\n[1]:http://zeppelin.apache.org/docs/0.9.0/interpreter/flink.html#flink-udf"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\nclass PythonLower(ScalarFunction):\n  def eval(self, s):\n    return s.lower()\n\nbt_env.register_function(\"python_lower\", udf(PythonLower(), DataTypes.STRING(), DataTypes.STRING()))"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT store_and_fwd_flag, python_lower(store_and_fwd_flag) as lower_store_and_fwd_flag FROM yellow_cab\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Window\n\n* [Window Aggregation][1]\n* [Different Types of Windows][2]\n\n[1]:https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/window-agg/\n[2]:https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/operators/windows/"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT AVG(trip_distance) as avg_trip_distance FROM yellow_cab GROUP BY TUMBLE(tpep_dropoff_datetime, INTERVAL \u002710\u0027 SECOND)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT AVG(fare_amount) as avg_fair_amount, VendorID FROM yellow_cab GROUP BY TUMBLE(tpep_dropoff_datetime, INTERVAL \u002710\u0027 SECOND), VendorID"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Join\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nDROP TABLE IF EXISTS vendor_details;\n\nCREATE TABLE vendor_details (\n    `VendorID` INT,\n    `VendorName` STRING\n) WITH (\n   \u0027connector\u0027\u003d\u0027filesystem\u0027,\n   \u0027path\u0027 \u003d \u0027s3://yellowcabsharkech/reference_data/vendor_ref.csv\u0027,\n   \u0027format\u0027 \u003d \u0027csv\u0027\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT vendor_details.VendorName, yellow_cab.passenger_count, yellow_cab.trip_distance, yellow_cab.fare_amount FROM yellow_cab \nJOIN vendor_details\nON yellow_cab.VendorID \u003d vendor_details.VendorID"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Write Data to S3"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nDROP TABLE IF EXISTS S3_yellow_cab;\n\nCREATE TABLE S3_yellow_cab (\n   `VendorID` INT,\n   `tpep_pickup_datetime` TIMESTAMP,\n   `tpep_dropoff_datetime` TIMESTAMP,\n   `passenger_count` INT,\n   `trip_distance` FLOAT,\n   `RatecodeID` INT,\n   `store_and_fwd_flag` STRING,\n   `PULocationID` INT,\n   `DOLocationID` INT,\n   `payment_type` INT,\n   `fare_amount` FLOAT,\n   `extra` FLOAT,\n   `mta_tax` FLOAT,\n   `tip_amount` FLOAT,\n   `tolls_amount` FLOAT,\n   `improvement_surcharge` FLOAT,\n   `total_amount` FLOAT,\n   `congestion_surcharge` FLOAT\n)\nPARTITIONED BY (VendorID)\nWITH (\n   \u0027connector\u0027\u003d\u0027filesystem\u0027,\n   \u0027path\u0027 \u003d \u0027s3://yellowcabsharkech/sql_output\u0027,\n   \u0027format\u0027 \u003d \u0027csv\u0027,\n   \u0027sink.partition-commit.policy.kind\u0027\u003d\u0027success-file\u0027,\n   \u0027sink.partition-commit,delay\u0027 \u003d \u00271 min\u0027\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\nst_env.get_config().get_configuration().set_string(\n    \"execution.checkpointing.mode\", \"EXACTLY_ONCE\"    \n)\n\nst_env.get_config().get_configuration().set_string(\n    \"execution.checkpointing.interval\", \"1min\"    \n)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nINSERT INTO S3_yellow_cab SELECT * FROM yellow_cab"
    }
  ]
}