{
  "metadata": {
    "name": "sql_1.13_DataGen",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Helpful Flink Resources\n\n* [Flink SQL API Examples. Common SQL query patterns. Windows, aggregation ...][1]\n* [AWS Online Tech Talks (YouTube). Streaming Analytics via. Kinesis Data Analytics Studio][2]\n* [Flink on Zeppelin 13 part YouTube Series][3]\n* [Flink on Zeppeling - Streaming Blog Post][4]\n* [Flink interpreter for Apache Zeppelin (Flink Documenation)][5]\n* [Flink SQL Documentation 1.13][6]\n* [Flink Kinesis Connector][7]\n* [Flink DataGen SQL Connector][8]\n\n[1]:https://awsfeed.com/whats-new/big-data/get-started-with-flink-sql-apis-in-amazon-kinesis-data-analytics-studio\n[2]:https://www.youtube.com/watch?v\u003dmRDst424mKY\n[3]:https://www.youtube.com/watch?v\u003dYxPo0Fosjjg\u0026list\u003dPL4oy12nnS7FFtg3KV1iS5vDb0pTz12VcX\n[4]:https://zjffdu.medium.com/flink-on-zeppelin-part-3-streaming-5fca1e16754\n[5]:http://zeppelin.apache.org/docs/0.9.0/interpreter/flink.html#streamexecutionenvironment-executionenvironment-streamtableenvir\n[6]:https://ci.apache.org/projects/flink/flink-docs-release-1.13/dev/table/sql/\n[7]:https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/kinesis/\n[8]:https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/datagen/"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### [Flink\u0027s Zeppelin Interpreters][1] \n\n| Name | Class | Description\n| --- | ----------- | --------|\n| %flink | FlinkInterpreter | Creates ExecutionEnvironment/StreamExecutionEnvironment/BatchTableEnvironment/StreamTableEnvironment and provides a Scala environment\n| %flink.pyflink | PyFlinkInterpreter | Provides a python environment\n| %flink.ipyflink | IPyFlinkInterpreter | Provides an ipython environment\n| %flink.ssql | FlinkStreamSqlInterpreter | Provides a stream sql environment\n| %flink.bsql | FlinkBatchSqlInterpreter | Provides a batch sql environment\n\n[1]:http://zeppelin.apache.org/docs/0.9.0/interpreter/flink.html#overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Create Table in Glue Data Catalog with the DataGen connector\n\nFor this notebook we will **NOT** be connecting to a Kinesis data stream or Apache Kafka topic. We will use the [DataGen SQL][1] connector to generate random data similar to the [NYC Taxi Cabs trip][2] data. \n\nIn the create table statement below ```\u0027connector\u0027 \u003d \u0027datagen\u0027``` instead of ```\u0027connector\u0027 \u003d \u0027kinesis\u0027``` or ```\u0027connector\u0027 \u003d \u0027kafka\u0027```\n\n*Note* the rate at which the DataGen table creates data can be alterted by adjusting ```\u0027rows-per-second\u0027 \u003d \u00271\u0027```\n\n[1]:https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/datagen/\n[2]:https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\nDROP TABLE IF EXISTS yellow_cab_1v13_DataGen;\n\nCREATE TABLE yellow_cab_1v13_DataGen (\n    `VendorID` INT,\n    `tpep_pickup_datetime` TIMESTAMP(3),\n    `tpep_dropoff_datetime` TIMESTAMP(3),\n    `passenger_count` INT,\n    `trip_distance` FLOAT,\n    `RatecodeID` INT,\n    `store_and_fwd_flag` AS \u0027T\u0027,\n    `PULocationID` INT,\n    `DOLocationID` INT,\n    `payment_type` INT,\n    `fare_amount` FLOAT,\n    `extra` FLOAT,\n    `mta_tax` FLOAT,\n    `tip_amount` FLOAT,\n    `tolls_amount` FLOAT,\n    `improvement_surcharge` FLOAT,\n    `total_amount` AS `fare_amount` + `extra` + `mta_tax` + `tip_amount` + `tolls_amount` + `improvement_surcharge`,\n    `congestion_surcharge` FLOAT,\n    WATERMARK FOR tpep_dropoff_datetime AS tpep_dropoff_datetime - INTERVAL \u00275\u0027 SECOND\n)\n WITH (\n    \u0027connector\u0027 \u003d \u0027datagen\u0027,\n    \u0027rows-per-second\u0027 \u003d \u00271\u0027,\n    \u0027fields.VendorID.min\u0027 \u003d \u00270\u0027,\n    \u0027fields.VendorID.max\u0027 \u003d \u00271\u0027,\n    \u0027fields.passenger_count.min\u0027 \u003d \u00271\u0027,\n    \u0027fields.passenger_count.max\u0027 \u003d \u00275\u0027,\n    \u0027fields.trip_distance.min\u0027 \u003d \u00271\u0027,\n    \u0027fields.trip_distance.max\u0027 \u003d \u002715\u0027,\n    \u0027fields.RatecodeID.min\u0027 \u003d \u00271\u0027,\n    \u0027fields.RatecodeID.max\u0027 \u003d \u00276\u0027,\n    \u0027fields.PULocationID.min\u0027 \u003d \u00271\u0027,\n    \u0027fields.PULocationID.max\u0027 \u003d \u0027251\u0027,\n    \u0027fields.DOLocationID.min\u0027 \u003d \u00271\u0027,\n    \u0027fields.DOLocationID.max\u0027 \u003d \u0027251\u0027,\n    \u0027fields.payment_type.min\u0027 \u003d \u00271\u0027,\n    \u0027fields.payment_type.max\u0027 \u003d \u00276\u0027,\n    \u0027fields.fare_amount.min\u0027 \u003d \u00276\u0027,\n    \u0027fields.fare_amount.max\u0027 \u003d \u0027150\u0027,\n    \u0027fields.extra.min\u0027 \u003d \u00270\u0027,\n    \u0027fields.extra.max\u0027 \u003d \u00271\u0027,\n    \u0027fields.mta_tax.min\u0027 \u003d \u00270\u0027,\n    \u0027fields.mta_tax.max\u0027 \u003d \u002710\u0027,\n    \u0027fields.tip_amount.min\u0027 \u003d \u00270\u0027,\n    \u0027fields.tip_amount.max\u0027 \u003d \u002710\u0027,\n    \u0027fields.tolls_amount.min\u0027 \u003d \u00270\u0027,\n    \u0027fields.tolls_amount.max\u0027 \u003d \u002715\u0027,\n    \u0027fields.improvement_surcharge.min\u0027 \u003d \u00270\u0027,\n    \u0027fields.improvement_surcharge.max\u0027 \u003d \u00270.30\u0027,\n    \u0027fields.congestion_surcharge.min\u0027 \u003d \u00270\u0027,\n    \u0027fields.congestion_surcharge.max\u0027 \u003d \u00274\u0027\n);"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\n-- SHOW CATALOGS;\n-- SHOW DATABASES;\n-- SHOW TABLES;\n\nDESCRIBE yellow_cab_1v13_DataGen;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Select all Fields from the yellow_cab_1v13_DataGen table"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT * FROM yellow_cab_1v13_DataGen;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Filter"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT * FROM yellow_cab_1v13_DataGen WHERE trip_distance \u003e 3.0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### User Defined Function"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\nclass PythonLower(ScalarFunction):\n  def eval(self, s):\n    return s.lower()\n\nbt_env.register_function(\"python_lower\", udf(PythonLower(), DataTypes.STRING(), DataTypes.STRING()))"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT store_and_fwd_flag, python_lower(store_and_fwd_flag) as lower_store_and_fwd_flag FROM yellow_cab_1v13_DataGen"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Window\n\n* [Window Aggregation][1]\n* [Different Types of Windows][2]\n\n[1]:https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/window-agg/\n[2]:https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/operators/windows/"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT AVG(trip_distance) as avg_trip_distance FROM yellow_cab_1v13_DataGen GROUP BY TUMBLE(tpep_dropoff_datetime, INTERVAL \u002710\u0027 SECOND)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT AVG(fare_amount) as avg_fair_amount, VendorID FROM yellow_cab_1v13_DataGen GROUP BY TUMBLE(tpep_dropoff_datetime, INTERVAL \u002710\u0027 SECOND), VendorID"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Join\n\n***Note*** before completing this next section ensure that you upload the [vendor_ref.csv][1] to an S3 bucket and update the ``` \u0027path\u0027 \u003d \u0027s3://yellowcabsharkech/reference_data/vendor_ref.csv\u0027 ``` in the paragraph below with the path of the CSV [vendor_ref.csv][1] file you uploaded.\n\n[1]:https://github.com/ev2900/Flink_Kinesis_Data_Analytics/blob/main/data/vendor_ref.csv"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nDROP TABLE IF EXISTS vendor_details_1v13;\n\nCREATE TABLE vendor_details_1v13 (\n    `VendorID` INT,\n    `VendorName` STRING\n) WITH (\n   \u0027connector\u0027\u003d\u0027filesystem\u0027,\n   \u0027path\u0027 \u003d \u0027s3://yellowcabsharkech/reference_data/vendor_ref.csv\u0027,\n   \u0027format\u0027 \u003d \u0027csv\u0027\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT vendor_details_1v13.VendorName, yellow_cab_1v13_DataGen.passenger_count, yellow_cab_1v13_DataGen.trip_distance, yellow_cab_1v13_DataGen.fare_amount FROM yellow_cab_1v13_DataGen \nJOIN vendor_details_1v13\nON yellow_cab_1v13_DataGen.VendorID \u003d vendor_details_1v13.VendorID"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Write Data to S3"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nDROP TABLE IF EXISTS S3_yellow_cab_1v13_DataGen;\n\nCREATE TABLE S3_yellow_cab_1v13_DataGen (\n   `VendorID` INT,\n   `tpep_pickup_datetime` TIMESTAMP,\n   `tpep_dropoff_datetime` TIMESTAMP,\n   `passenger_count` INT,\n   `trip_distance` FLOAT,\n   `RatecodeID` INT,\n   `store_and_fwd_flag` STRING,\n   `PULocationID` INT,\n   `DOLocationID` INT,\n   `payment_type` INT,\n   `fare_amount` FLOAT,\n   `extra` FLOAT,\n   `mta_tax` FLOAT,\n   `tip_amount` FLOAT,\n   `tolls_amount` FLOAT,\n   `improvement_surcharge` FLOAT,\n   `total_amount` FLOAT,\n   `congestion_surcharge` FLOAT,\n   `arrival_time` TIMESTAMP(3),\n   `shard_id` VARCHAR(128),\n   `sequence_number` VARCHAR(128)\n)\nPARTITIONED BY (VendorID)\nWITH (\n   \u0027connector\u0027\u003d\u0027filesystem\u0027,\n   \u0027path\u0027 \u003d \u0027s3://yellowcabsharkech/sql_output_1v13_DataGen\u0027,\n   \u0027format\u0027 \u003d \u0027csv\u0027,\n   \u0027sink.partition-commit.policy.kind\u0027\u003d\u0027success-file\u0027,\n   \u0027sink.partition-commit.delay\u0027 \u003d \u00271 min\u0027\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\nst_env.get_config().get_configuration().set_string(\n    \"execution.checkpointing.mode\", \"EXACTLY_ONCE\"    \n)\n\nst_env.get_config().get_configuration().set_string(\n    \"execution.checkpointing.interval\", \"1min\"    \n)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nINSERT INTO S3_yellow_cab_1v13 SELECT * FROM S3_yellow_cab_1v13_DataGen"
    }
  ]
}