{
  "metadata": {
    "name": "sql_1.13",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Helpful Flink Resources\n\n* [Flink SQL API Examples. Common SQL query patterns. Windows, aggregation ...][1]\n* [AWS Online Tech Talks (YouTube). Streaming Analytics via. Kinesis Data Analytics Studio][2]\n* [Flink on Zeppelin 13 part YouTube Series][3]\n* [Flink on Zeppeling - Streaming Blog Post][4]\n* [Flink interpreter for Apache Zeppelin (Flink Documenation)][5]\n* [Flink SQL Documentation 1.13][6]\n* [Flink Kinesis Connector][7]\n\n[1]:https://awsfeed.com/whats-new/big-data/get-started-with-flink-sql-apis-in-amazon-kinesis-data-analytics-studio\n[2]:https://www.youtube.com/watch?v\u003dmRDst424mKY\n[3]:https://www.youtube.com/watch?v\u003dYxPo0Fosjjg\u0026list\u003dPL4oy12nnS7FFtg3KV1iS5vDb0pTz12VcX\n[4]:https://zjffdu.medium.com/flink-on-zeppelin-part-3-streaming-5fca1e16754\n[5]:http://zeppelin.apache.org/docs/0.9.0/interpreter/flink.html#streamexecutionenvironment-executionenvironment-streamtableenvir\n[6]:https://ci.apache.org/projects/flink/flink-docs-release-1.13/dev/table/sql/\n[7]:https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/kinesis/"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### [Flink\u0027s Zeppelin Interpreters][1] \n\n| Name | Class | Description\n| --- | ----------- | --------|\n| %flink | FlinkInterpreter | Creates ExecutionEnvironment/StreamExecutionEnvironment/BatchTableEnvironment/StreamTableEnvironment and provides a Scala environment\n| %flink.pyflink | PyFlinkInterpreter | Provides a python environment\n| %flink.ipyflink | IPyFlinkInterpreter | Provides an ipython environment\n| %flink.ssql | FlinkStreamSqlInterpreter | Provides a stream sql environment\n| %flink.bsql | FlinkBatchSqlInterpreter | Provides a batch sql environment\n\n[1]:http://zeppelin.apache.org/docs/0.9.0/interpreter/flink.html#overview"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Create Table in Glue Data Catalog\n\n* [Flink Supported Data Types][1]\n\n[1]:https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/types/"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\nDROP TABLE IF EXISTS yellow_cab_1v13;\n\nCREATE TABLE yellow_cab_1v13 (\n   `VendorID` INT,\n   `tpep_pickup_datetime` TIMESTAMP(3),\n   `tpep_dropoff_datetime` TIMESTAMP(3),\n   `passenger_count` INT,\n   `trip_distance` FLOAT,\n   `RatecodeID` INT,\n   `store_and_fwd_flag` STRING,\n   `PULocationID` INT,\n   `DOLocationID` INT,\n   `payment_type` INT,\n   `fare_amount` FLOAT,\n   `extra` FLOAT,\n   `mta_tax` FLOAT,\n   `tip_amount` FLOAT,\n   `tolls_amount` FLOAT,\n   `improvement_surcharge` FLOAT,\n   `total_amount` FLOAT,\n   `congestion_surcharge` FLOAT,\n   `arrival_time` TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027 VIRTUAL,\n   `shard_id` VARCHAR(128) NOT NULL METADATA FROM \u0027shard-id\u0027 VIRTUAL,\n   `sequence_number` VARCHAR(128) NOT NULL METADATA FROM \u0027sequence-number\u0027 VIRTUAL,\n   WATERMARK FOR tpep_dropoff_datetime AS tpep_dropoff_datetime - INTERVAL \u00275\u0027 SECOND\n) \n WITH (\n   \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n   \u0027stream\u0027 \u003d \u0027yellow-cab-trip\u0027,\n   \u0027aws.region\u0027 \u003d \u0027us-east-1\u0027,\n   \u0027scan.stream.initpos\u0027 \u003d \u0027LATEST\u0027,\n   \u0027format\u0027 \u003d \u0027json\u0027\n);"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\n-- SHOW CATALOGS;\n-- SHOW DATABASES;\n-- SHOW TABLES;\n\nDESCRIBE yellow_cab_1v13;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Select all Fields from the yellow-cab-trip stream"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT * FROM yellow_cab_1v13;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Filter"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT * FROM yellow_cab_1v13 WHERE trip_distance \u003e 3.0 "
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### User Defined Function"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\nclass PythonLower(ScalarFunction):\n  def eval(self, s):\n    return s.lower()\n\nbt_env.register_function(\"python_lower\", udf(PythonLower(), DataTypes.STRING(), DataTypes.STRING()))"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT store_and_fwd_flag, python_lower(store_and_fwd_flag) as lower_store_and_fwd_flag FROM yellow_cab_1v13\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Window\n\n* [Window Aggregation][1]\n* [Different Types of Windows][2]\n\n[1]:https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/table/sql/queries/window-agg/\n[2]:https://ci.apache.org/projects/flink/flink-docs-master/docs/dev/datastream/operators/windows/\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT AVG(trip_distance) as avg_trip_distance FROM yellow_cab_1v13 GROUP BY TUMBLE(tpep_dropoff_datetime, INTERVAL \u002710\u0027 SECOND)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT AVG(fare_amount) as avg_fair_amount, VendorID FROM yellow_cab_1v13 GROUP BY TUMBLE(tpep_dropoff_datetime, INTERVAL \u002710\u0027 SECOND), VendorID"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Join\n\n***Note*** before completing this next section ensure that you upload the [vendor_ref.csv][1] to an S3 bucket and update the ``` \u0027path\u0027 \u003d \u0027s3://yellowcabsharkech/reference_data/vendor_ref.csv\u0027 ``` in the paragraph below with the path of the CSV [vendor_ref.csv][1] file you uploaded.\n\n[1]:https://github.com/ev2900/Flink_Kinesis_Data_Analytics/blob/main/data/vendor_ref.csv"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nDROP TABLE IF EXISTS vendor_details_1v13;\n\nCREATE TABLE vendor_details_1v13 (\n    `VendorID` INT,\n    `VendorName` STRING\n) WITH (\n   \u0027connector\u0027\u003d\u0027filesystem\u0027,\n   \u0027path\u0027 \u003d \u0027s3://yellowcabsharkech/reference_data/vendor_ref.csv\u0027,\n   \u0027format\u0027 \u003d \u0027csv\u0027\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT vendor_details_1v13.VendorName, yellow_cab_1v13.passenger_count, yellow_cab_1v13.trip_distance, yellow_cab_1v13.fare_amount FROM yellow_cab_1v13 \nJOIN vendor_details_1v13\nON yellow_cab_1v13.VendorID \u003d vendor_details_1v13.VendorID\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Write Data to S3"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nDROP TABLE IF EXISTS S3_yellow_cab_1v13;\n\nCREATE TABLE S3_yellow_cab_1v13 (\n   `VendorID` INT,\n   `tpep_pickup_datetime` TIMESTAMP,\n   `tpep_dropoff_datetime` TIMESTAMP,\n   `passenger_count` INT,\n   `trip_distance` FLOAT,\n   `RatecodeID` INT,\n   `store_and_fwd_flag` STRING,\n   `PULocationID` INT,\n   `DOLocationID` INT,\n   `payment_type` INT,\n   `fare_amount` FLOAT,\n   `extra` FLOAT,\n   `mta_tax` FLOAT,\n   `tip_amount` FLOAT,\n   `tolls_amount` FLOAT,\n   `improvement_surcharge` FLOAT,\n   `total_amount` FLOAT,\n   `congestion_surcharge` FLOAT,\n   `arrival_time` TIMESTAMP(3),\n   `shard_id` VARCHAR(128),\n   `sequence_number` VARCHAR(128)\n)\nPARTITIONED BY (VendorID)\nWITH (\n   \u0027connector\u0027\u003d\u0027filesystem\u0027,\n   \u0027path\u0027 \u003d \u0027s3://yellowcabsharkech/sql_output_1v13\u0027,\n   \u0027format\u0027 \u003d \u0027csv\u0027,\n   \u0027sink.partition-commit.policy.kind\u0027\u003d\u0027success-file\u0027,\n   \u0027sink.partition-commit.delay\u0027 \u003d \u00271 min\u0027\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.pyflink\n\nst_env.get_config().get_configuration().set_string(\n    \"execution.checkpointing.mode\", \"EXACTLY_ONCE\"    \n)\n\nst_env.get_config().get_configuration().set_string(\n    \"execution.checkpointing.interval\", \"1min\"    \n)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nINSERT INTO S3_yellow_cab_1v13 SELECT * FROM yellow_cab_1v13"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Start (restart) processing the yellow-cab-trip data stream from the begining of the kinesis data stream\n\n```\n\u0027scan.stream.initpos\u0027 \u003d \u0027TRIM_HORIZON\u0027\n```\n\nin the create table statement. Reference [start reading position for more information][1]\n\n[1]:https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/table/kinesis/#start-reading-position"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql\n\nDROP TABLE IF EXISTS yellow_cab_1v13_Restart;\n\nCREATE TABLE yellow_cab_1v13_Restart (\n   `VendorID` INT,\n   `tpep_pickup_datetime` TIMESTAMP(3),\n   `tpep_dropoff_datetime` TIMESTAMP(3),\n   `passenger_count` INT,\n   `trip_distance` FLOAT,\n   `RatecodeID` INT,\n   `store_and_fwd_flag` STRING,\n   `PULocationID` INT,\n   `DOLocationID` INT,\n   `payment_type` INT,\n   `fare_amount` FLOAT,\n   `extra` FLOAT,\n   `mta_tax` FLOAT,\n   `tip_amount` FLOAT,\n   `tolls_amount` FLOAT,\n   `improvement_surcharge` FLOAT,\n   `total_amount` FLOAT,\n   `congestion_surcharge` FLOAT,\n   `arrival_time` TIMESTAMP(3) METADATA FROM \u0027timestamp\u0027 VIRTUAL,\n   `shard_id` VARCHAR(128) NOT NULL METADATA FROM \u0027shard-id\u0027 VIRTUAL,\n   `sequence_number` VARCHAR(128) NOT NULL METADATA FROM \u0027sequence-number\u0027 VIRTUAL,\n   WATERMARK FOR tpep_dropoff_datetime AS tpep_dropoff_datetime - INTERVAL \u00275\u0027 SECOND\n) \n WITH (\n   \u0027connector\u0027 \u003d \u0027kinesis\u0027,\n   \u0027stream\u0027 \u003d \u0027yellow-cab-trip\u0027,\n   \u0027aws.region\u0027 \u003d \u0027us-east-1\u0027,\n   \u0027scan.stream.initpos\u0027 \u003d \u0027TRIM_HORIZON\u0027,\n   \u0027format\u0027 \u003d \u0027json\u0027\n);"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%flink.ssql(type\u003dupdate)\n\nSELECT * FROM yellow_cab_1v13_Restart;"
    }
  ]
}